{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone project Pricify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to build a model to predict a price for item by picture. As OfferUp says that \"With a single snap, you can take a photo of an item and instantly circulate it to people nearby.\", it sounds interesting to suggest the price, so you can take a photo of item and decide either you should sell it or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OfferUp scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrap_offerup.rb  - HTTP Request to https://offerupnow.com/ and scrap the recent offers page by page until the date limit was reached. Each offer was stored into file in JSON format. \n",
    "\n",
    "normalize_scraped.rb - Spliting and combining offers into 3 JSON files, I got:\n",
    "offerup-data\n",
    "* items.json 605 Mb 380,107 items\n",
    "* owners.json 141.2 Mb ? item owners\n",
    "* images.json 214.1 Mb ? links to item images\n",
    "\n",
    "download_images.rb - 3 size of images organizes into subfolders with offer-id name.\n",
    "* detail\n",
    "* full\n",
    "* list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./scrap_offerup.rb -c /tmp/cookie.txt -s ./scraped -t 0 -n 20000 -p 0.5 | tee scrap.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/items.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = cdf.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 380107 to 300304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf.drop_duplicates(subset='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from item.json. We got 300,304 rows and 34 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns are: \n",
    "\n",
    "* category - object (calculate how many categories, split into separate table?)\n",
    "* condition\t - int (40, 100 ? looks like categorical)\n",
    "* description - text\n",
    "* distance\t- distance from logged user, not applicable\n",
    "* get_full_url\t- link to offer\n",
    "* get_img_medium_height\t400\n",
    "* get_img_medium_width\t300\n",
    "* get_img_permalink_large\n",
    "* get_img_permalink_medium\n",
    "* get_img_permalink_small\n",
    "* get_img_small_height\n",
    "* get_img_small_width\n",
    "* get_small_square_thumbanil\n",
    "* id\t65194613\n",
    "* image\tNone\n",
    "* image_mob_det_hd\n",
    "* image_mob_list_hd\n",
    "* latitude\t47.8426\n",
    "* listing_type\t2\n",
    "* location_name\tLynnwood, WA\n",
    "* longitude\t-122.295\n",
    "* owner_id\t6787474\n",
    "* payable\tFalse\n",
    "* post_date\t2015-12-19T19:38:50.398Z\n",
    "* post_from_store_address\tLynnwood, WA\n",
    "* price\t25\n",
    "* priority\t100\n",
    "* reservable\tFalse\n",
    "* reserved\tFalse\n",
    "* review_status\t2\n",
    "* sold_date\tNone\n",
    "* sold_offer_id\tNaN\n",
    "* state\t3\n",
    "* title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Category\n",
    " - category 380107 non-null object - no missing values\n",
    " - Create two features with category data - category_id and category_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['category_id'] =  cdf.category.apply(lambda x: int(x['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['category_name'] =  cdf.category.apply(lambda x: str(x['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "cdf = cdf[cdf['price'] < 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_counts = cdf['category_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cat_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 37 categories of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_counts.plot(kind='density')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To simplify the model, let's decrease the number of categories. \n",
    "\n",
    "Baby & Kids              37114\n",
    "Clothing & Shoes         27743\n",
    "Games & Toys              8149  \n",
    "\n",
    "\n",
    "Furniture                22857\n",
    "Household                21974\n",
    "Home & Garden            9529\n",
    "\n",
    "Cell Phones              14307 - select phones\n",
    "Electronics              14967 - select phones\n",
    "\n",
    "I'm going to train models on each of these groups separately, maybe using different models for each of three categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Selected features: \n",
    "features = ['id', 'description', 'title', 'category_id', 'category_name', 'price']\n",
    "fdf = cdf[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phones_category = fdf[fdf['category_name'] == 'Cell Phones']\n",
    "phones_category.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phones_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apparel_category = fdf[fdf['category_name'].isin(['Baby & Kids', 'Clothing & Shoes'])]\n",
    "apparel_category.shape\n",
    "#apparel_category.id.to_csv('apparel_category.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apparel_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_category = fdf[fdf['category_name'].isin(['Furniture', 'Household', 'Home & Garden'])]\n",
    "house_category.shape\n",
    "#house_category.id.to_csv('house_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let see the most common words in titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phones_base = cdf[cdf['category_name'] == 'Cell Phones']\n",
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), phones_base.title.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Most common in description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), phones_base.description.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), apparel_category.title.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), apparel_category.description.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), house_category.title.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), house_category.description.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_words_check(title, words):\n",
    "    if len(set(title.split()).intersection(words)) > 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phone_words = set(['unlocked', 'iphone', 'galaxy', 'samsung', 'note', 'phone', 'lg', 'htc', 'verizon', 't-mobile', 'at&t', \n",
    "              'tmobile','nokia', 'mobile', 'sony', 'motorola', 'unlocled', 'lumia', 'smart', 'phones', 'nexus'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#phones_category = cdf[cdf['category_name'].isin(['Cell Phones', 'Electronics'])]\n",
    "#phones_category = phones_category[phones_category['title'].apply(lambda x: key_words_check(x, phone_words))]\n",
    "#phones_category.shape\n",
    "#phones_category.id.to_csv('phones_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phones_category = fdf[fdf['category_name'] == 'Cell Phones']\n",
    "phones_category.id.to_csv('phones_category.csv', index=False)\n",
    "phones_category.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition\n",
    "  \n",
    "* condition 380107 non-null int64 - no missing values\n",
    "* value range: [0, 20, 40, 60, 80, 100], so it looks like categorical.\n",
    "* Create 6 features with conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_counts = cdf['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_counts.plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in  [0, 20, 40, 60, 80, 100]:\n",
    "    cdf['condition_' + str(i)] =  cdf.condition.apply(lambda x: 1 if x == i else 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description, title\n",
    "  \n",
    "* description                   380107 non-null object - no missing values\n",
    "* title                         380107 non-null object\n",
    "* vectorize with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance\n",
    "  \n",
    "* distance                      380107 non-null int64 - no missing values\n",
    "* It looks like distance was calculated based on location from cookie file, so from my home, it's not relevant in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Owner\n",
    "* owner_id                      380107 non-null int64\n",
    "* Information about owner looks valueble, but in case of consignments it's not likely that the same person would sell the same tipe of item again and again. I will skip owners information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priority\n",
    "* priority                      380107 non-null int64\n",
    "* All observation have the same priority 100, will not use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reservable, reserved\n",
    "* reservable                    380107 non-null bool\n",
    "* reserved                      380107 non-null bool\n",
    "* Only two offer has reservable=True, will not use it\n",
    "* All offers have reserved=False, will not use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['payable'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price\n",
    "  \n",
    "* price                         380107 non-null float64 - no missing values\n",
    "* target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save for later\n",
    "\n",
    "* latitude                      380107 non-null float64\n",
    "* listing_type                  380107 non-null int64\n",
    "* location_name                 380107 non-null object\n",
    "* longitude                     380107 non-null float64\n",
    "* payable                       380107 non-null bool\n",
    "* post_date                     380107 non-null object\n",
    "* post_from_store_address       377435 non-null object\n",
    "* review_status                 285697 non-null float64\n",
    "* sold_date                     6486 non-null object\n",
    "* sold_offer_id                 3862 non-null float64\n",
    "* state                         380107 non-null int64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n",
    "\n",
    "Doesn't look relevant\n",
    "\n",
    "* get_full_url                  380107 non-null object\n",
    "* get_img_medium_height         380107 non-null int64\n",
    "* get_img_medium_width          380107 non-null int64\n",
    "* get_img_permalink_large       380107 non-null object\n",
    "* get_img_permalink_medium      380107 non-null object\n",
    "* get_img_permalink_small       380107 non-null object\n",
    "* get_img_small_height          380107 non-null int64\n",
    "* get_img_small_width           380107 non-null int64\n",
    "* get_small_square_thumbanil    380107 non-null object\n",
    "* id                            380107 non-null int64\n",
    "* image                         159857 non-null object\n",
    "* image_mob_det_hd              380107 non-null object\n",
    "* image_mob_list_hd             380107 non-null object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### As a result I will use next data:\n",
    "\n",
    "['id', 'description', 'title', 'category_id', 'category_name', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model - category classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will build recommendation model for each category. Lets return to the whole dataset and try to predict item class based on 'title', 'description' and 'deep_features' from product picture. For this we need to train our model on whole data set. There are two options for the target:\n",
    "1. We stay with current categories, so try to predict one from ['Cell Phones', 'Baby & Kids', 'Clothing & Shoes', 'Games & Toys', 'Furniture', 'Household', 'Home & Garden'] and then select a next model based on these on from 7 category. \n",
    "2. Create new target with values ['phones', 'apparel', 'house']\n",
    "Let's comapare these two models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, split data into test and train subsets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_category_name(category):\n",
    "    if category == 'Cell Phones':\n",
    "        return 'phones'\n",
    "    elif category in ['Furniture', 'Household', 'Home & Garden']:\n",
    "        return 'home'\n",
    "    elif category in ['Baby & Kids', 'Clothing & Shoes']:\n",
    "        return 'apparel'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tk/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "fdf['category'] =  fdf.category_name.apply(lambda x: set_category_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_categories = fdf[fdf.category != 'other']\n",
    "categories = [\n",
    "        'home',\n",
    "        'phones',\n",
    "        'apparel'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_target_small = all_categories['category_name']\n",
    "all_target_big = all_categories['category']\n",
    "all_text = all_categories.title + \" \" + all_categories.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "all_big_train, all_big_test, target_big_train, target_big_test = train_test_split(all_text, all_target_big, test_size=0.2, random_state=55)\n",
    "all_small_train, all_small_test, target_small_train, target_small_test = train_test_split(all_text, all_target_small, test_size=0.2, random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both models features will be the same. \n",
    "\n",
    "#### Bag of Words or “Bag of n-grams” representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = all_big_train, all_big_test, target_big_train, target_big_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107384    Kids Size 12 Nike Cleats Size 12 cleats. Worn ...\n",
       "366942                           Roxy boots 7.5 arm grey \\n\n",
       "5080      48\" Glass Top Dining Set Beautiful glass top d...\n",
       "37006        Home decor Hand Crafted Home sweet Home sign. \n",
       "219907                                        China buffet \n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a sparse vectorizer\n",
      "n_samples: 106432, n_features: 1000\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "n_samples: 26608, n_features: 1000\n",
      "\n",
      "done in 0.314882s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split a training set and a test set\n",
    "\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "#                                 stop_words='english')\n",
    "vectorizer = TfidfVectorizer(input='content', lowercase=True, tokenizer=None,\n",
    "                                    stop_words='english', use_idf=True,\n",
    "                                    max_features=1000, ngram_range=(1, 3))\n",
    "X_train = vectorizer.fit_transform(text_train)\n",
    "duration = time() - t0\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(text_test)\n",
    "duration = time() - t0\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "ch2 = SelectKBest(chi2)\n",
    "X_train = ch2.fit_transform(X_train, y_train)\n",
    "X_test = ch2.transform(X_test)\n",
    "if feature_names:\n",
    "    # keep selected feature names\n",
    "    feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Benchmark classifiers\n",
    "def benchmark(clf, ):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        \n",
    "        print(\"top 10 keywords per class:\")\n",
    "        for i, category in enumerate(categories):\n",
    "            top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "            print(trim(\"%s: %s\"\n",
    "                    % (category, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    \n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=categories))\n",
    "\n",
    "    \n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='lsqr',\n",
      "        tol=0.01)\n",
      "train time: 1.209s\n",
      "test time:  0.002s\n",
      "accuracy:   0.674\n",
      "dimensionality: 10\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "home: tmobile samsung unlocked iphone galaxy mobile phone case samsung galaxy...\n",
      "phones: size mobile tmobile samsung galaxy iphone unlocked phone case samsung...\n",
      "apparel: samsung galaxy size case phone unlocked galaxy iphone samsung mobile...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.40      0.56     12905\n",
      "     phones       0.56      0.95      0.71     10921\n",
      "    apparel       0.95      0.85      0.90      2782\n",
      "\n",
      "avg / total       0.77      0.67      0.66     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5199  7625    81]\n",
      " [  507 10373    41]\n",
      " [   12   398  2372]]\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=50, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "train time: 2.164s\n",
      "test time:  0.002s\n",
      "accuracy:   0.669\n",
      "dimensionality: 10\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "home: tmobile unlocked iphone galaxy samsung galaxy samsung case mobile size ...\n",
      "phones: iphone case tmobile samsung mobile unlocked galaxy size phone samsung...\n",
      "apparel: size phone samsung galaxy samsung mobile case iphone unlocked galaxy...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.88      0.40      0.55     12905\n",
      "     phones       0.57      0.95      0.71     10921\n",
      "    apparel       0.92      0.82      0.86      2782\n",
      "\n",
      "avg / total       0.75      0.67      0.65     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5217  7552   136]\n",
      " [  533 10321    67]\n",
      " [  206   308  2268]]\n",
      "\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "train time: 28.765s\n",
      "test time:  0.515s\n",
      "accuracy:   0.662\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.90      0.37      0.53     12905\n",
      "     phones       0.55      0.95      0.70     10921\n",
      "    apparel       0.95      0.86      0.90      2782\n",
      "\n",
      "avg / total       0.76      0.66      0.64     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 4812  8011    82]\n",
      " [  466 10410    45]\n",
      " [   52   343  2387]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tk/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/ridge.py:299: UserWarning: In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "  warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"\n",
      "/Users/tk/anaconda2/lib/python2.7/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.676s\n",
      "test time:  0.001s\n",
      "accuracy:   0.677\n",
      "dimensionality: 10\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "home: tmobile samsung unlocked iphone galaxy phone case mobile samsung galaxy...\n",
      "phones: size mobile galaxy unlocked tmobile samsung iphone phone case samsung...\n",
      "apparel: samsung galaxy size case mobile phone galaxy iphone unlocked samsung...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.40      0.56     12905\n",
      "     phones       0.57      0.95      0.71     10921\n",
      "    apparel       0.95      0.87      0.91      2782\n",
      "\n",
      "avg / total       0.77      0.68      0.66     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5223  7603    79]\n",
      " [  508 10367    46]\n",
      " [   15   347  2420]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 1.868s\n",
      "test time:  0.003s\n",
      "accuracy:   0.674\n",
      "dimensionality: 10\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "home: tmobile unlocked samsung iphone phone galaxy case samsung galaxy mobile...\n",
      "phones: size mobile iphone phone galaxy samsung tmobile unlocked case samsung...\n",
      "apparel: size samsung galaxy case phone unlocked galaxy samsung tmobile mobil...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.40      0.56     12905\n",
      "     phones       0.56      0.95      0.71     10921\n",
      "    apparel       0.94      0.86      0.90      2782\n",
      "\n",
      "avg / total       0.77      0.67      0.65     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5164  7650    91]\n",
      " [  496 10375    50]\n",
      " [   13   370  2399]]\n",
      "\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l1', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.173s\n",
      "test time:  0.002s\n",
      "accuracy:   0.677\n",
      "dimensionality: 10\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "home: tmobile samsung unlocked iphone galaxy phone case mobile samsung galaxy...\n",
      "phones: size mobile unlocked galaxy tmobile samsung iphone phone case samsung...\n",
      "apparel: samsung galaxy size case mobile phone iphone galaxy unlocked samsung...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.41      0.56     12905\n",
      "     phones       0.57      0.95      0.71     10921\n",
      "    apparel       0.95      0.87      0.91      2782\n",
      "\n",
      "avg / total       0.77      0.68      0.66     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5229  7604    72]\n",
      " [  508 10367    46]\n",
      " [   18   345  2419]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 2.051s\n",
      "test time:  0.002s\n",
      "accuracy:   0.675\n",
      "dimensionality: 10\n",
      "density: 0.866667\n",
      "top 10 keywords per class:\n",
      "home: phone tmobile iphone galaxy samsung case unlocked mobile samsung galaxy...\n",
      "phones: size mobile samsung iphone galaxy phone unlocked tmobile case samsung...\n",
      "apparel: size samsung galaxy case mobile phone unlocked galaxy iphone samsung...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.40      0.56     12905\n",
      "     phones       0.57      0.95      0.71     10921\n",
      "    apparel       0.94      0.87      0.91      2782\n",
      "\n",
      "avg / total       0.77      0.68      0.66     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5178  7634    93]\n",
      " [  499 10373    49]\n",
      " [   13   351  2418]]\n",
      "\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 2.638s\n",
      "test time:  0.002s\n",
      "accuracy:   0.674\n",
      "dimensionality: 10\n",
      "density: 0.966667\n",
      "top 10 keywords per class:\n",
      "home: tmobile iphone unlocked samsung phone galaxy case mobile samsung galaxy...\n",
      "phones: size mobile iphone galaxy phone samsung tmobile unlocked case samsung...\n",
      "apparel: size samsung galaxy case unlocked phone galaxy samsung mobile tmobil...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.40      0.56     12905\n",
      "     phones       0.56      0.95      0.71     10921\n",
      "    apparel       0.94      0.86      0.90      2782\n",
      "\n",
      "avg / total       0.77      0.67      0.65     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5166  7648    91]\n",
      " [  497 10374    50]\n",
      " [   13   369  2400]]\n",
      "\n",
      "================================================================================\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "train time: 0.068s\n",
      "test time:  0.004s\n",
      "accuracy:   0.671\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.40      0.56     12905\n",
      "     phones       0.56      0.95      0.71     10921\n",
      "    apparel       0.95      0.82      0.88      2782\n",
      "\n",
      "avg / total       0.77      0.67      0.65     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5192  7635    78]\n",
      " [  505 10364    52]\n",
      " [   12   479  2291]]\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.167s\n",
      "test time:  0.002s\n",
      "accuracy:   0.532\n",
      "dimensionality: 10\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "home: tmobile samsung galaxy unlocked samsung galaxy iphone phone mobile case...\n",
      "phones: unlocked tmobile samsung galaxy galaxy mobile samsung iphone phone ca...\n",
      "apparel: size samsung galaxy tmobile samsung galaxy phone case unlocked mobil...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.53      0.99      0.69     12905\n",
      "     phones       0.07      0.01      0.01     10921\n",
      "    apparel       0.99      0.46      0.62      2782\n",
      "\n",
      "avg / total       0.39      0.53      0.41     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[12800    98     7]\n",
      " [10830    88     3]\n",
      " [  360  1153  1269]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.179s\n",
      "test time:  0.002s\n",
      "accuracy:   0.675\n",
      "dimensionality: 10\n",
      "density: 1.000000\n",
      "top 10 keywords per class:\n",
      "home: tmobile samsung galaxy unlocked samsung galaxy iphone phone mobile case...\n",
      "phones: unlocked tmobile samsung galaxy galaxy mobile samsung iphone phone ca...\n",
      "apparel: size phone case samsung galaxy galaxy iphone samsung unlocked tmobil...\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.40      0.56     12905\n",
      "     phones       0.57      0.95      0.71     10921\n",
      "    apparel       0.94      0.86      0.90      2782\n",
      "\n",
      "avg / total       0.77      0.68      0.66     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5196  7605   104]\n",
      " [  506 10369    46]\n",
      " [   15   370  2397]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tk/anaconda2/lib/python2.7/site-packages/sklearn/svm/classes.py:197: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/Users/tk/anaconda2/lib/python2.7/site-packages/sklearn/utils/__init__.py:93: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(steps=[('feature_selection', LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0)), ('classification', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "train time: 3.060s\n",
      "test time:  0.004s\n",
      "accuracy:   0.677\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       home       0.91      0.40      0.56     12905\n",
      "     phones       0.57      0.95      0.71     10921\n",
      "    apparel       0.95      0.87      0.91      2782\n",
      "\n",
      "avg / total       0.77      0.68      0.66     26608\n",
      "\n",
      "confusion matrix:\n",
      "[[ 5223  7603    79]\n",
      " [  508 10367    46]\n",
      " [   15   348  2419]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tk/anaconda2/lib/python2.7/site-packages/sklearn/utils/__init__.py:93: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        #(PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        #(KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n",
    "                                            dual=False, tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
    "  ('classification', LinearSVC())\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_text_features(X_train, X_test, Y, Y_test):\n",
    "for max_features in [1000, 5000, 10000]:\n",
    "    vectorizer = TfidfVectorizer(lowercase=True,stop_words='english', max_features=max_features, ngram_range=(1, 3))\n",
    "        \n",
    "    train_tf_idf = vectorizer.fit_transform(X_train)\n",
    "    test_tf_idf = vectorizer.transform(X_test)\n",
    "\n",
    "    for alpha in [1.0, 0.5, 0.1, 1e-09, 0.0]:\n",
    "            # initiate model as per grid params\n",
    "            desc_nb_model = MultinomialNB(alpha=alpha)\n",
    "\n",
    "            desc_nb_model.fit(desc_tfidf_train, y_train)\n",
    "\n",
    "            print 'accuracy: {}, alpha: {}, max_features: {}'.format(desc_nb_model.score(desc_tfidf_test, y_test), alpha, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_nlp(X_train, X_test, y_train, y_test, textual_data='desc_init'):\n",
    "    \"\"\"Grid search TfIdf vectorizer and Multinomial NB for best accuracy on text data.\"\"\"\n",
    "    print textual_data\n",
    "    for max_features in [30000, 20000, 10000]:\n",
    "        # initiate vectorizer as per grid params\n",
    "        desc_vect = TfidfVectorizer(input='content', lowercase=True, tokenizer=None,\n",
    "                                    stop_words='english', use_idf=True,\n",
    "                                    max_features=max_features, ngram_range=(1, 3))\n",
    "        desc_tfidf_train = desc_vect.fit_transform(X_train[textual_data])\n",
    "        desc_tfidf_test = desc_vect.transform(X_test[textual_data])\n",
    "\n",
    "        for alpha in [1.0, 0.5, 0.1, 1e-09, 0.0]:\n",
    "            # initiate model as per grid params\n",
    "            desc_nb_model = MultinomialNB(alpha=alpha)\n",
    "\n",
    "            desc_nb_model.fit(desc_tfidf_train, y_train)\n",
    "\n",
    "            print 'accuracy: {}, alpha: {}, max_features: {}'.format(desc_nb_model.score(desc_tfidf_test, y_test), alpha, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to GraphLab Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "gl.canvas.set_target('ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apparel = gl.SFrame(apparel_category)\n",
    "house = gl.SFrame(house_category)\n",
    "phones = gl.SFrame(phones_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the last step I'm going to use graphlab.nearest_neighbors to get top 5 nearest offers to display for user and choose median price value as recommendation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gramms = ['new', 'used', 'unlocked', 'good condition', 'great condition', 'very good condition', 'never used']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phones['title_word_count'] = gl.text_analytics.tf_idf(phones['title'])\n",
    "#phones['desc_word_count'] = gl.text_analytics.count_words(phones['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl = gl.text_analytics.count_ngrams(phones['description'], 2)\n",
    "c = Counter()\n",
    "for row in pl: \n",
    "    for key, value in row.iteritems():\n",
    "        c[key] += value\n",
    "c.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl = gl.text_analytics.count_ngrams(apparel['title'], 2)\n",
    "c = Counter()\n",
    "for row in pl: \n",
    "    for key, value in row.iteritems():\n",
    "        c[key] += value\n",
    "c.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apparel_model = gl.nearest_neighbors.create(apparel_category,features=['deep_features'],label='id')\n",
    "house_model = gl.nearest_neighbors.create(house_category,features=['deep_features'],label='id')\n",
    "phone_model = gl.nearest_neighbors.create(phone_category,features=['deep_features'],label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional features for each category from title and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TF/IDF\n",
    "    #vectorizer1 = TfidfVectorizer(encoding='english',\n",
    "    #                            stop_words='english',\n",
    "    #                            strip_accents=\"ascii\",\n",
    "    #                          # token_pattern=r'\\w{3,}',\n",
    "    #                           max_features=100)\n",
    "\n",
    "    text_vec = df['description'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())\n",
    "    text_vec1 = zip(text_vec, df['name'], df['org_name'], df['payee_name'], df['org_desc'])\n",
    "    text_vec1 = [ ''.join(ln) for ln in text_vec1]\n",
    "    count_char = pd.Series(text_vec1)\n",
    "    df[\"Numberof!\"]    = count_char.apply(lambda x: x.count(\"!\"))\n",
    "    df[\"NumberofCaps\"] = count_char.apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    \n",
    "    tfidf_vec = joblib.load(tfidf_file)\n",
    "\n",
    "    r       = tfidf_vec.transform(text_vec1)\n",
    "    columns = tfidf_vec.get_feature_names()\n",
    "    columns = [ 'tfidf_'+c for c in columns]\n",
    "    temp    = pd.DataFrame(r.toarray(),columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
