{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone project Pricify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to build a model to predict a price for item by picture. As OfferUp says that \"With a single snap, you can take a photo of an item and instantly circulate it to people nearby.\", it sounds interesting to suggest the price, so you can take a photo of item and decide either you should sell it or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OfferUp scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrap_offerup.rb  - HTTP Request to https://offerupnow.com/ and scrap the recent offers page by page until the date limit was reached. Each offer was stored into file in JSON format. \n",
    "\n",
    "normalize_scraped.rb - Spliting and combining offers into 3 JSON files, I got:\n",
    "offerup-data\n",
    "* items.json 605 Mb 380,107 items\n",
    "* owners.json 141.2 Mb ? item owners\n",
    "* images.json 214.1 Mb ? links to item images\n",
    "\n",
    "download_images.rb - 3 size of images organizes into subfolders with offer-id name.\n",
    "* detail\n",
    "* full\n",
    "* list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "./scrap_offerup.rb -c /tmp/cookie.txt -s ./scraped -t 0 -n 20000 -p 0.5 | tee scrap.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/items.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = cdf.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From 380107 to 300304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdf.drop_duplicates(subset='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from item.json. We got 300,304 rows and 34 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns are: \n",
    "\n",
    "* category - object (calculate how many categories, split into separate table?)\n",
    "* condition\t - int (40, 100 ? looks like categorical)\n",
    "* description - text\n",
    "* distance\t- distance from logged user, not applicable\n",
    "* get_full_url\t- link to offer\n",
    "* get_img_medium_height\t400\n",
    "* get_img_medium_width\t300\n",
    "* get_img_permalink_large\n",
    "* get_img_permalink_medium\n",
    "* get_img_permalink_small\n",
    "* get_img_small_height\n",
    "* get_img_small_width\n",
    "* get_small_square_thumbanil\n",
    "* id\t65194613\n",
    "* image\tNone\n",
    "* image_mob_det_hd\n",
    "* image_mob_list_hd\n",
    "* latitude\t47.8426\n",
    "* listing_type\t2\n",
    "* location_name\tLynnwood, WA\n",
    "* longitude\t-122.295\n",
    "* owner_id\t6787474\n",
    "* payable\tFalse\n",
    "* post_date\t2015-12-19T19:38:50.398Z\n",
    "* post_from_store_address\tLynnwood, WA\n",
    "* price\t25\n",
    "* priority\t100\n",
    "* reservable\tFalse\n",
    "* reserved\tFalse\n",
    "* review_status\t2\n",
    "* sold_date\tNone\n",
    "* sold_offer_id\tNaN\n",
    "* state\t3\n",
    "* title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Category\n",
    " - category 380107 non-null object - no missing values\n",
    " - Create two features with category data - category_id and category_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['category_id'] =  cdf.category.apply(lambda x: int(x['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['category_name'] =  cdf.category.apply(lambda x: str(x['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "cdf = cdf[cdf['price'] < 1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_counts = cdf['category_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cat_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have 37 categories of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_counts.plot(kind='density')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To simplify the model, let's decrease the number of categories. \n",
    "\n",
    "Baby & Kids              37114\n",
    "Clothing & Shoes         27743\n",
    "Games & Toys              8149  \n",
    "\n",
    "\n",
    "Furniture                22857\n",
    "Household                21974\n",
    "Home & Garden            9529\n",
    "\n",
    "Cell Phones              14307 - select phones\n",
    "Electronics              14967 - select phones\n",
    "\n",
    "I'm going to train models on each of these groups separately, maybe using different models for each of three categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Selected features: \n",
    "features = ['id', 'description', 'title', 'category_id', 'category_name', 'price']\n",
    "fdf = cdf[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phones_category = fdf[fdf['category_name'] == 'Cell Phones']\n",
    "phones_category.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phones_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apparel_category = fdf[fdf['category_name'].isin(['Baby & Kids', 'Clothing & Shoes'])]\n",
    "apparel_category.shape\n",
    "#apparel_category.id.to_csv('apparel_category.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apparel_category.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_category = fdf[fdf['category_name'].isin(['Furniture', 'Household', 'Home & Garden'])]\n",
    "house_category.shape\n",
    "#house_category.id.to_csv('house_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "house_category.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let see the most common words in titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phones_base = cdf[cdf['category_name'] == 'Cell Phones']\n",
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), phones_base.title.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Most common in description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), phones_base.description.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), apparel_category.title.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), apparel_category.description.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), house_category.title.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = map(lambda x: x.split(), house_category.description.tolist())\n",
    "c = Counter([item.lower() for sublist in l for item in sublist])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def key_words_check(title, words):\n",
    "    if len(set(title.split()).intersection(words)) > 0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phone_words = set(['unlocked', 'iphone', 'galaxy', 'samsung', 'note', 'phone', 'lg', 'htc', 'verizon', 't-mobile', 'at&t', \n",
    "              'tmobile','nokia', 'mobile', 'sony', 'motorola', 'unlocled', 'lumia', 'smart', 'phones', 'nexus'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#phones_category = cdf[cdf['category_name'].isin(['Cell Phones', 'Electronics'])]\n",
    "#phones_category = phones_category[phones_category['title'].apply(lambda x: key_words_check(x, phone_words))]\n",
    "#phones_category.shape\n",
    "#phones_category.id.to_csv('phones_category.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phones_category = fdf[fdf['category_name'] == 'Cell Phones']\n",
    "phones_category.id.to_csv('phones_category.csv', index=False)\n",
    "phones_category.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition\n",
    "  \n",
    "* condition 380107 non-null int64 - no missing values\n",
    "* value range: [0, 20, 40, 60, 80, 100], so it looks like categorical.\n",
    "* Create 6 features with conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_counts = cdf['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition_counts.plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in  [0, 20, 40, 60, 80, 100]:\n",
    "    cdf['condition_' + str(i)] =  cdf.condition.apply(lambda x: 1 if x == i else 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description, title\n",
    "  \n",
    "* description                   380107 non-null object - no missing values\n",
    "* title                         380107 non-null object\n",
    "* vectorize with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance\n",
    "  \n",
    "* distance                      380107 non-null int64 - no missing values\n",
    "* It looks like distance was calculated based on location from cookie file, so from my home, it's not relevant in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Owner\n",
    "* owner_id                      380107 non-null int64\n",
    "* Information about owner looks valueble, but in case of consignments it's not likely that the same person would sell the same tipe of item again and again. I will skip owners information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priority\n",
    "* priority                      380107 non-null int64\n",
    "* All observation have the same priority 100, will not use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reservable, reserved\n",
    "* reservable                    380107 non-null bool\n",
    "* reserved                      380107 non-null bool\n",
    "* Only two offer has reservable=True, will not use it\n",
    "* All offers have reserved=False, will not use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cdf['payable'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price\n",
    "  \n",
    "* price                         380107 non-null float64 - no missing values\n",
    "* target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save for later\n",
    "\n",
    "* latitude                      380107 non-null float64\n",
    "* listing_type                  380107 non-null int64\n",
    "* location_name                 380107 non-null object\n",
    "* longitude                     380107 non-null float64\n",
    "* payable                       380107 non-null bool\n",
    "* post_date                     380107 non-null object\n",
    "* post_from_store_address       377435 non-null object\n",
    "* review_status                 285697 non-null float64\n",
    "* sold_date                     6486 non-null object\n",
    "* sold_offer_id                 3862 non-null float64\n",
    "* state                         380107 non-null int64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n",
    "\n",
    "Doesn't look relevant\n",
    "\n",
    "* get_full_url                  380107 non-null object\n",
    "* get_img_medium_height         380107 non-null int64\n",
    "* get_img_medium_width          380107 non-null int64\n",
    "* get_img_permalink_large       380107 non-null object\n",
    "* get_img_permalink_medium      380107 non-null object\n",
    "* get_img_permalink_small       380107 non-null object\n",
    "* get_img_small_height          380107 non-null int64\n",
    "* get_img_small_width           380107 non-null int64\n",
    "* get_small_square_thumbanil    380107 non-null object\n",
    "* id                            380107 non-null int64\n",
    "* image                         159857 non-null object\n",
    "* image_mob_det_hd              380107 non-null object\n",
    "* image_mob_list_hd             380107 non-null object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### As a result I will use next data:\n",
    "\n",
    "['id', 'description', 'title', 'category_id', 'category_name', 'price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Model - category classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will build recommendation model for each category. Lets return to the whole dataset and try to predict item class based on 'title', 'description' and 'deep_features' from product picture. For this we need to train our model on whole data set. There are two options for the target:\n",
    "1. We stay with current categories, so try to predict one from ['Cell Phones', 'Baby & Kids', 'Clothing & Shoes', 'Games & Toys', 'Furniture', 'Household', 'Home & Garden'] and then select a next model based on these on from 7 category. \n",
    "2. Create new target with values ['phones', 'apparel', 'house']\n",
    "Let's comapare these two models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, split data into test and train subsets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_category_name(category):\n",
    "    if category == 'Cell Phones':\n",
    "        return 'phones'\n",
    "    elif category in ['Furniture', 'Household', 'Home & Garden']:\n",
    "        return 'home'\n",
    "    elif category in ['Baby & Kids', 'Clothing & Shoes']:\n",
    "        return 'apparel'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdf['category'] =  fdf.category_name.apply(lambda x: set_category_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_categories = fdf[fdf.category != 'other']\n",
    "categories = [\n",
    "        'home',\n",
    "        'phones',\n",
    "        'apparel'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_target_small = all_categories['category_name']\n",
    "all_target_big = all_categories['category']\n",
    "all_text = all_categories.title + \" \" + all_categories.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "all_big_train, all_big_test, target_big_train, target_big_test = train_test_split(all_text, all_target_big, test_size=0.2, random_state=55)\n",
    "all_small_train, all_small_test, target_small_train, target_small_test = train_test_split(all_text, all_target_small, test_size=0.2, random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both models features will be the same. \n",
    "\n",
    "#### Bag of Words or “Bag of n-grams” representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_test = all_big_train, all_big_test, target_big_train, target_big_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split a training set and a test set\n",
    "\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "#                                 stop_words='english')\n",
    "vectorizer = TfidfVectorizer(input='content', lowercase=True, tokenizer=None,\n",
    "                                    stop_words='english', use_idf=True,\n",
    "                                    max_features=1000, ngram_range=(1, 3))\n",
    "X_train = vectorizer.fit_transform(text_train)\n",
    "duration = time() - t0\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(text_test)\n",
    "duration = time() - t0\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "ch2 = SelectKBest(chi2)\n",
    "X_train = ch2.fit_transform(X_train, y_train)\n",
    "X_test = ch2.transform(X_test)\n",
    "if feature_names:\n",
    "    # keep selected feature names\n",
    "    feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Benchmark classifiers\n",
    "def benchmark(clf, ):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        \n",
    "        print(\"top 10 keywords per class:\")\n",
    "        for i, category in enumerate(categories):\n",
    "            top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "            print(trim(\"%s: %s\"\n",
    "                    % (category, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    \n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=categories))\n",
    "\n",
    "    \n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        #(KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n",
    "                                            dual=False, tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
    "  ('classification', LinearSVC())\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_text_features(X_train, X_test, Y, Y_test):\n",
    "for max_features in [1000, 5000, 10000]:\n",
    "    vectorizer = TfidfVectorizer(lowercase=True,stop_words='english', max_features=max_features, ngram_range=(1, 3))\n",
    "        \n",
    "    train_tf_idf = vectorizer.fit_transform(X_train)\n",
    "    test_tf_idf = vectorizer.transform(X_test)\n",
    "\n",
    "    for alpha in [1.0, 0.5, 0.1, 1e-09, 0.0]:\n",
    "            # initiate model as per grid params\n",
    "            desc_nb_model = MultinomialNB(alpha=alpha)\n",
    "\n",
    "            desc_nb_model.fit(desc_tfidf_train, y_train)\n",
    "\n",
    "            print 'accuracy: {}, alpha: {}, max_features: {}'.format(desc_nb_model.score(desc_tfidf_test, y_test), alpha, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_nlp(X_train, X_test, y_train, y_test, textual_data='desc_init'):\n",
    "    \"\"\"Grid search TfIdf vectorizer and Multinomial NB for best accuracy on text data.\"\"\"\n",
    "    print textual_data\n",
    "    for max_features in [30000, 20000, 10000]:\n",
    "        # initiate vectorizer as per grid params\n",
    "        desc_vect = TfidfVectorizer(input='content', lowercase=True, tokenizer=None,\n",
    "                                    stop_words='english', use_idf=True,\n",
    "                                    max_features=max_features, ngram_range=(1, 3))\n",
    "        desc_tfidf_train = desc_vect.fit_transform(X_train[textual_data])\n",
    "        desc_tfidf_test = desc_vect.transform(X_test[textual_data])\n",
    "\n",
    "        for alpha in [1.0, 0.5, 0.1, 1e-09, 0.0]:\n",
    "            # initiate model as per grid params\n",
    "            desc_nb_model = MultinomialNB(alpha=alpha)\n",
    "\n",
    "            desc_nb_model.fit(desc_tfidf_train, y_train)\n",
    "\n",
    "            print 'accuracy: {}, alpha: {}, max_features: {}'.format(desc_nb_model.score(desc_tfidf_test, y_test), alpha, max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch to GraphLab Create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "gl.canvas.set_target('ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apparel = gl.SFrame(apparel_category)\n",
    "house = gl.SFrame(house_category)\n",
    "phones = gl.SFrame(phones_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the last step I'm going to use graphlab.nearest_neighbors to get top 5 nearest offers to display for user and choose median price value as recommendation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gramms = ['new', 'used', 'unlocked', 'good condition', 'great condition', 'very good condition', 'never used']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phones['title_word_count'] = gl.text_analytics.tf_idf(phones['title'])\n",
    "#phones['desc_word_count'] = gl.text_analytics.count_words(phones['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl = gl.text_analytics.count_ngrams(phones['description'], 2)\n",
    "c = Counter()\n",
    "for row in pl: \n",
    "    for key, value in row.iteritems():\n",
    "        c[key] += value\n",
    "c.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl = gl.text_analytics.count_ngrams(apparel['title'], 2)\n",
    "c = Counter()\n",
    "for row in pl: \n",
    "    for key, value in row.iteritems():\n",
    "        c[key] += value\n",
    "c.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apparel_model = gl.nearest_neighbors.create(apparel_category,features=['deep_features'],label='id')\n",
    "house_model = gl.nearest_neighbors.create(house_category,features=['deep_features'],label='id')\n",
    "phone_model = gl.nearest_neighbors.create(phone_category,features=['deep_features'],label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional features for each category from title and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TF/IDF\n",
    "    #vectorizer1 = TfidfVectorizer(encoding='english',\n",
    "    #                            stop_words='english',\n",
    "    #                            strip_accents=\"ascii\",\n",
    "    #                          # token_pattern=r'\\w{3,}',\n",
    "    #                           max_features=100)\n",
    "\n",
    "    text_vec = df['description'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())\n",
    "    text_vec1 = zip(text_vec, df['name'], df['org_name'], df['payee_name'], df['org_desc'])\n",
    "    text_vec1 = [ ''.join(ln) for ln in text_vec1]\n",
    "    count_char = pd.Series(text_vec1)\n",
    "    df[\"Numberof!\"]    = count_char.apply(lambda x: x.count(\"!\"))\n",
    "    df[\"NumberofCaps\"] = count_char.apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    \n",
    "    tfidf_vec = joblib.load(tfidf_file)\n",
    "\n",
    "    r       = tfidf_vec.transform(text_vec1)\n",
    "    columns = tfidf_vec.get_feature_names()\n",
    "    columns = [ 'tfidf_'+c for c in columns]\n",
    "    temp    = pd.DataFrame(r.toarray(),columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
